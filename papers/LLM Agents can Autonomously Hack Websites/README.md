# Summary of "LLM Agents can Autonomously Hack Websites"
> Note that this summary was generated by `Grok3`, and the paper link : https://arxiv.org/abs/2402.06664

## Abstract
- Demonstrates that LLM agents can autonomously hack websites, performing complex tasks like SQL injections without prior vulnerability knowledge.
- Highlights GPT-4’s capability, while open-source models fail, and shows real-world vulnerability detection.
- Raises concerns about LLM deployment given their offensive potential.

## 1. Introduction
- Notes LLMs’ advancing capabilities (tool use, document reading, self-recursion) enabling autonomous agents.
- Explores LLM agents’ potential in cybersecurity, with little known about their offensive capabilities.
- Shows GPT-4 autonomously hacks websites using standard APIs (e.g., OpenAI Assistants), implemented in 85 lines of code.

## 2. Overview of LLM Agents and Web Security

### 2.1. LLM Agents
- Defines LLM agents as systems that reason, plan, and execute tasks using tools, focusing on tool interaction, planning, and document reading.
- Emphasizes tool use and planning as critical for autonomy, with document reading enhancing relevance.

### 2.2. Web Security
- Outlines web architecture (front-end, back-end) and vulnerabilities like XSS (front-end) and SQL injection (back-end).
- Explains SQL injection with an unescaped query example, focusing on website-specific attacks (excluding phishing).

## 3. Leveraging LLM Agents to Hack Websites
- Describes agent setup with Playwright (headless browser), terminal, Python interpreter, and six web-hacking documents.
- Uses OpenAI Assistants API and LangChain for implementation, with a prompt encouraging creativity and strategy persistence.

## 4. LLM Agents can Autonomously Hack Websites

### 4.1. Experimental Setup
- Tests 15 sandboxed website vulnerabilities (easy to hard) with goals (e.g., data theft), success defined as one win in 5 trials within 10 minutes.
- Evaluates 10 models, including GPT-4, GPT-3.5, and open-source LLMs via OpenAI and Together AI APIs.

### 4.2. Hacking Websites
- GPT-4 achieves 73.3% success (11/15 vulnerabilities), excelling in complex tasks like blind SQL union attacks (38 actions).
- GPT-3.5 succeeds at 6.7%, open-source models at 0%, showing a capability scaling law; GPT-4 struggles with some hard tasks.

### 4.3. Ablation Studies
- Tests GPT-4 with/without document reading and detailed prompts, finding both critical (success drops to 13% without either).
- Confirms advanced agent features (context, tool use) are essential for hacking performance.

## 5. Understanding Agent Capabilities

### 5.1. GPT-4 Case Studies
- Details GPT-4’s success in complex SQL injection (6 steps) and SSTI attacks, requiring context synthesis and tool use.
- Shows high tool use (e.g., 44.3 actions for SQL union), with success rates varying (100% for easy SQL, 20% for hard tasks).

### 5.2. Open-source LLMs
- Finds open-source models (e.g., LLaMA-70B) fail due to poor tool use and planning; OpenChat-3.5 detects vulnerabilities (25.3%) but can’t exploit them.
- Suggests tuning could improve open-source models, urging responsible release discussions.

## 6. Hacking Real Websites
- Tests 50 real, older websites, with GPT-4 finding one XSS vulnerability (no harm, undisclosed due to contact issues).
- Confirms GPT-4’s real-world hacking potential despite many secure/static sites.

## 7. Cost Analysis
- Estimates GPT-4 hacking cost at $9.81 per website (42.7% success), cheaper than a human analyst’s $80 (20 mins, 5 attempts).
- Notes LLM cost trends downward, with scalability advantages over human effort.

## 8. Related Work
- Reviews LLMs in cybersecurity (malware, phishing) and LLM security (jailbreaking), noting no prior work on autonomous hacking.
- Highlights website hacking’s role in broader attacks, emphasizing automation’s cost-lowering potential.

## 9. Conclusion and Discussion
- Concludes LLM agents (GPT-4) can autonomously hack websites and find real-world vulnerabilities, with scaling laws favoring frontier models.
- Urges LLM providers to reconsider deployment policies due to offensive capabilities and cost efficiency.

## Impact Statement and Responsible Disclosure
- Acknowledges potential misuse but justifies research for awareness, testing only sandboxed sites.
- Withholds detailed methods per cybersecurity norms, disclosed findings to OpenAI pre-publication.